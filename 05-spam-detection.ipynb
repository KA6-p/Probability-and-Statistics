{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cfc1afb-072e-413f-93ff-31753a274fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c4ad8c-0caa-45e7-bc92-6612488f0cc0",
   "metadata": {},
   "source": [
    "## Collecting the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c39386e0-3249-4c6d-b626-017718030bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "spam = [\n",
    "     \"To use your credit, click the new WAP link in the next years txt message or click here\", \n",
    "    \"Thanks for your subscription to New Ringtone UK your new mobile will be charged £5/month Please confirm annoncement by replying\", \n",
    "    \"As a valued customer, I am pleased to advise you that following recent delivery waiting review of your Mob No. you are awarded with. Call us to review.\", \n",
    "    \"Please call our new customer service representative on\", \n",
    "    \"We are trying to contact you. Last weekends customer draw shows that you won a £1000 prize GUARANTEED. Calling years\", \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22ed2dc9-095c-42da-be0f-3331114fad69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# leaving one sentence from spam for testing our model later \n",
    "spam_test = [\"Customer service annoncement. You have a New Years delivery waiting for you. click\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "164092d2-fc56-4b35-8d94-ce0369cf6c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "non = [\n",
    "    \"I don't think he goes to usf, he lives around here though\", \n",
    "    \"New car and house for my parents. i have only new job in hand\", \n",
    "    \"Great escape. I fancy the bridge but needs her lager. See you tomorrow\", \n",
    "    \"Tired. I haven't slept well the past few nights.\",\n",
    "    \"Too late. I said i have the website. I didn't i have or dont have the slippers\", \n",
    "    \"I might come by tonight then if my class lets out early\", \n",
    "    \"Jos ask if u wana meet up?\", \n",
    "    \"That would be great. We'll be at the Guild. We can try meeting with the customer on Bristol road or somewhere\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03496efd-a3a1-489e-b1c1-ab7d58c41901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# another sentence from non for testing our model \n",
    "spam_test_2 = [\"That would be great. We'll be at the Guild. We can try meeting with the customer on Bristol road or somewhere\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed09b21-a28e-4491-8ce1-6690ba184b03",
   "metadata": {},
   "source": [
    "## Basic Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68e1071c-cbe8-48b4-9428-a0529eacceb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\dell\\onedrive\\desktop\\gitdemo\\gensim_env\\lib\\site-packages (4.3.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in c:\\users\\dell\\onedrive\\desktop\\gitdemo\\gensim_env\\lib\\site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in c:\\users\\dell\\onedrive\\desktop\\gitdemo\\gensim_env\\lib\\site-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\dell\\onedrive\\desktop\\gitdemo\\gensim_env\\lib\\site-packages (from gensim) (7.3.0.post1)\n",
      "Requirement already satisfied: wrapt in c:\\users\\dell\\onedrive\\desktop\\gitdemo\\gensim_env\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a7c387a-deca-472c-87d0-05722bbda610",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from gensim.parsing.porter import PorterStemmer\n",
    "from gensim.utils import tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ddd70024-92fa-4b2f-87e6-6dc4deb3e1ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I might come by tonight then if my class lets out early\n",
      "\n",
      "\n",
      "I come tonight class lets early\n",
      "\n",
      "\n",
      "i might come by tonight then if my class lets out earli\n",
      "\n",
      "\n",
      "['I', 'might', 'come', 'by', 'tonight', 'then', 'if', 'my', 'class', 'lets', 'out', 'early']\n"
     ]
    }
   ],
   "source": [
    "test_sentence = non[4]\n",
    "test_sentence = non[5]\n",
    "# test_sentence = spam[1]\n",
    "\n",
    "print(test_sentence)\n",
    "print(\"\\n\")\n",
    "\n",
    "removed_stops = remove_stopwords(test_sentence)\n",
    "print(removed_stops)\n",
    "print(\"\\n\")\n",
    "\n",
    "p = PorterStemmer()\n",
    "stemmed = p.stem(test_sentence)\n",
    "print(stemmed)\n",
    "print(\"\\n\")\n",
    "\n",
    "tokens = list(tokenize(test_sentence))\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da68c7b-7719-419d-9785-31abef1b8811",
   "metadata": {},
   "source": [
    "## Create a dictionary of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a99771d9-a583-4c85-9b08-4fd1d2c2e72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentence(sentence):\n",
    "    p = PorterStemmer()\n",
    "    removed_stops = remove_stopwords(sentence)\n",
    "    stemmed = p.stem(removed_stops)\n",
    "    tokens=tokenize(stemmed)\n",
    "    return list(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c6aa2c87-d180-4a66-bcb3-c087e2154723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized spam: [['to', 'use', 'credit', 'click', 'new', 'wap', 'link', 'years', 'txt', 'message', 'click'], ['thanks', 'subscription', 'new', 'ringtone', 'uk', 'new', 'mobile', 'charged', 'month', 'please', 'confirm', 'annoncement', 'repli'], ['as', 'valued', 'customer', 'i', 'pleased', 'advise', 'following', 'recent', 'delivery', 'waiting', 'review', 'mob', 'no', 'awarded', 'with', 'call', 'review'], ['please', 'new', 'customer', 'service', 'repres'], ['we', 'trying', 'contact', 'you', 'last', 'weekends', 'customer', 'draw', 'shows', 'won', 'prize', 'guaranteed', 'calling', 'year']]\n",
      "Tokenized non: [['i', 'don', 't', 'think', 'goes', 'usf', 'l'], ['new', 'car', 'house', 'parents', 'new', 'job', 'hand'], ['great', 'escape', 'i', 'fancy', 'bridge', 'needs', 'lager', 'see', 'tomorrow'], ['tired', 'i', 'haven', 't', 'slept', 'past', 'nights'], ['too', 'late', 'i', 'said', 'website', 'i', 'didn', 't', 'dont', 'slipp'], ['i', 'come', 'tonight', 'class', 'lets', 'earli'], ['jos', 'ask', 'u', 'wana', 'meet', 'up'], ['that', 'great', 'we', 'll', 'guild', 'we', 'try', 'meeting', 'customer', 'bristol', 'road']]\n",
      "Dictionary: {'following', 'last', 'parents', 'tomorrow', 'mobile', 'haven', 'to', 'road', 'wana', 'mob', 'service', 'house', 'pleased', 'no', 'contact', 'slept', 'late', 'great', 'use', 'nights', 'needs', 'come', 'job', 'guild', 'uk', 'usf', 'dont', 'new', 'link', 'click', 'delivery', 'tonight', 'shows', 'slipp', 'as', 'you', 'month', 'ask', 'annoncement', 'prize', 'lets', 'guaranteed', 'up', 'see', 'too', 'won', 'customer', 'meeting', 'jos', 'advise', 'that', 'l', 'i', 'confirm', 'draw', 'class', 'u', 'website', 'please', 'earli', 'hand', 'recent', 'years', 'year', 'message', 'weekends', 'goes', 'review', 'waiting', 'with', 'think', 'ringtone', 'thanks', 'escape', 'we', 't', 'don', 'repres', 'try', 'awarded', 'didn', 'txt', 'calling', 'bridge', 'fancy', 'repli', 'bristol', 'said', 'subscription', 'll', 'valued', 'tired', 'call', 'lager', 'credit', 'wap', 'car', 'past', 'charged', 'trying', 'meet'}\n"
     ]
    }
   ],
   "source": [
    "dictionary = set()\n",
    "spams_tokenized = []\n",
    "nons_tokenized = []\n",
    "\n",
    "for sentence in spam:\n",
    "    sentence_tokens = tokenize_sentence(sentence)\n",
    "    spams_tokenized.append(sentence_tokens)\n",
    "    dictionary = dictionary.union(sentence_tokens)\n",
    "\n",
    "for sentence in non:\n",
    "    sentence_tokens = tokenize_sentence(sentence)\n",
    "    nons_tokenized.append(sentence_tokens)\n",
    "    dictionary = dictionary.union(sentence_tokens)\n",
    "\n",
    "\n",
    "print(\"Tokenized spam:\",spams_tokenized)\n",
    "print(\"Tokenized non:\",nons_tokenized)\n",
    "print(\"Dictionary:\",dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55a24fa-4442-42d4-b886-cb841b4fda90",
   "metadata": {},
   "source": [
    "## Basic Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "153c16d4-8f47-486d-ba7c-dc112a144d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of words: 101\n"
     ]
    }
   ],
   "source": [
    "total_word_count = len(dictionary)\n",
    "total_spam_messages = len(spams_tokenized)\n",
    "total_all_messages = len(spams_tokenized) + len(nons_tokenized)\n",
    "\n",
    "print(\"Total Number of words:\",total_word_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "faba8339-9ac3-428f-9774-aa49c04cd266",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_spam = total_spam_messages/total_all_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b537911a-2007-4f99-b17f-a8f99acfb5b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(spam): 0.38461538461538464\n"
     ]
    }
   ],
   "source": [
    "print(\"P(spam):\",p_spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "66660803-0c37-4b2f-80a7-87c50bd720b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to count occurances\n",
    "def count_word_in_messages(word,messages):\n",
    "    total_count=0\n",
    "    for msg in messages:\n",
    "        if word in msg:\n",
    "            total_count+=1\n",
    "    return total_count            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d19c30b-6881-44d8-8d15-954812b1f382",
   "metadata": {},
   "source": [
    "## The Actual Probability Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4de76bc5-f9cf-44f8-bb7d-2d9fec4f0c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['that', 'great', 'we', 'll', 'guild', 'we', 'try', 'meeting', 'customer', 'bristol', 'road']\n",
      "--------------------\n",
      "Running for word: that\n",
      "P(w | spam): 0.0\n",
      "P(w): 0.07692307692307693\n",
      "P(spam)   = 0.38461538461538464\n",
      "P(spam | w) = 0.0\n",
      "\n",
      "--------------------\n",
      "Running for word: great\n",
      "P(w | spam): 0.0\n",
      "P(w): 0.15384615384615385\n",
      "P(spam)   = 0.38461538461538464\n",
      "P(spam | w) = 0.0\n",
      "\n",
      "--------------------\n",
      "Running for word: we\n",
      "P(w | spam): 0.2\n",
      "P(w): 0.15384615384615385\n",
      "P(spam)   = 0.38461538461538464\n",
      "P(spam | w) = 0.5\n",
      "\n",
      "--------------------\n",
      "Running for word: ll\n",
      "P(w | spam): 0.0\n",
      "P(w): 0.07692307692307693\n",
      "P(spam)   = 0.38461538461538464\n",
      "P(spam | w) = 0.0\n",
      "\n",
      "--------------------\n",
      "Running for word: guild\n",
      "P(w | spam): 0.0\n",
      "P(w): 0.07692307692307693\n",
      "P(spam)   = 0.38461538461538464\n",
      "P(spam | w) = 0.0\n",
      "\n",
      "--------------------\n",
      "Running for word: we\n",
      "P(w | spam): 0.2\n",
      "P(w): 0.15384615384615385\n",
      "P(spam)   = 0.38461538461538464\n",
      "P(spam | w) = 0.5\n",
      "\n",
      "--------------------\n",
      "Running for word: try\n",
      "P(w | spam): 0.0\n",
      "P(w): 0.07692307692307693\n",
      "P(spam)   = 0.38461538461538464\n",
      "P(spam | w) = 0.0\n",
      "\n",
      "--------------------\n",
      "Running for word: meeting\n",
      "P(w | spam): 0.0\n",
      "P(w): 0.07692307692307693\n",
      "P(spam)   = 0.38461538461538464\n",
      "P(spam | w) = 0.0\n",
      "\n",
      "--------------------\n",
      "Running for word: customer\n",
      "P(w | spam): 0.6\n",
      "P(w): 0.3076923076923077\n",
      "P(spam)   = 0.38461538461538464\n",
      "P(spam | w) = 0.75\n",
      "\n",
      "--------------------\n",
      "Running for word: bristol\n",
      "P(w | spam): 0.0\n",
      "P(w): 0.07692307692307693\n",
      "P(spam)   = 0.38461538461538464\n",
      "P(spam | w) = 0.0\n",
      "\n",
      "--------------------\n",
      "Running for word: road\n",
      "P(w | spam): 0.0\n",
      "P(w): 0.07692307692307693\n",
      "P(spam)   = 0.38461538461538464\n",
      "P(spam | w) = 0.0\n",
      "\n",
      "P(spam | all_words) = 0.0\n"
     ]
    }
   ],
   "source": [
    "final_prob = 1\n",
    "\n",
    "for test_sentence in spam_test_2:\n",
    "    test_sentence = tokenize_sentence(test_sentence)\n",
    "    print(test_sentence)\n",
    "\n",
    "    for word in test_sentence:\n",
    "        print(\"--------------------\")\n",
    "        print(\"Running for word:\",word)\n",
    "\n",
    "        # Find P(w / spam)\n",
    "        spam_count = count_word_in_messages(word,spams_tokenized)\n",
    "        p_w_spam = spam_count / total_spam_messages\n",
    "        print(\"P(w | spam):\",p_w_spam)\n",
    "\n",
    "        # Find P(w)\n",
    "        w_count = count_word_in_messages(word,spams_tokenized)\n",
    "        w_count += count_word_in_messages(word,nons_tokenized)\n",
    "        p_w = w_count / total_all_messages\n",
    "        print(\"P(w):\",p_w)\n",
    "        \n",
    "        # Find P(spam / w)\n",
    "        p_spam_w = (p_w_spam * p_spam) / p_w\n",
    "        print(\"P(spam)   =\",p_spam)\n",
    "        print(\"P(spam | w) =\",p_spam_w)\n",
    "        print(\"\")\n",
    "        final_prob *=p_spam_w\n",
    "\n",
    "    print(\"P(spam | all_words) =\",final_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce2c728-f1a2-4458-aa17-0afeceb81d14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
